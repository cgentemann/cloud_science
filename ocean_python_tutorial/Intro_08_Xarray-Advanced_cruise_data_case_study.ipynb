{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SC57 - Working with big, multi-dimensional geoscientific datasets in Python: a tutorial introduction to xarray](http://meetingorganizer.copernicus.org/EGU2017/session/25651)  \n",
    "  \n",
    "  \n",
    "Original notebook by [Stephan Hoyer](http://stephanhoyer.com), Rossbypalooza, 2016.  \n",
    "Modified by Edward Byers, Matthew Gidden and [Fabien Maussion](http://fabienmaussion.info/) for EGU General Assembly 2017, Vienna, Austria\n",
    "Modified by C. Gentemann for GHRSST Science Team Tutorial 2019, Rome, Italy\n",
    "Modified by C.Gentemann for future tutorials  \n",
    "    \n",
    "  \n",
    "  \n",
    "**Convenors**\n",
    "* [Dr Chelle Gentemann](mailto:gentemann@esr.org)    - Earth and Space Research, USA\n",
    "* [Dr Marisol Garcia-Reyes](mailto:marisolgr@faralloninstitute.org)  - Farallon Institute, USA \n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of this tutorial\n",
    "\n",
    "1. Opening data\n",
    "1. Collocating satellite data with a cruise dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Key features of `xarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "## Import python packages\n",
    "\n",
    "You are going to want numpy, pandas, matplotlib.pyplot and xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "from lxml import objectify\n",
    "#for search capabilites import podaacpy\n",
    "import podaac.podaac as podaac\n",
    "from podaac import drive as podaacdrive\n",
    "import podaac.podaac_utils as putil\n",
    "# then create an instance of the Podaac class\n",
    "p = podaac.Podaac()\n",
    "with open('./podaac.ini', 'r') as f:\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read_file(f)\n",
    "    d = podaacdrive.Drive(None, \n",
    "                          config['drive']['urs_username'], \n",
    "                          config['drive']['urs_password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A nice cartopy tutorial is [here](http://earthpy.org/tag/visualization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocating Saildrone cruise data with MUR SSTs \n",
    "\n",
    "* read in the Saildrone data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if offline use first url\n",
    "#url = './data/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc'\n",
    "url = 'https://podaac-opendap.jpl.nasa.gov/opendap/hyrax/allData/insitu/L2/saildrone/Baja/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc'\n",
    "ds_usv = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NCEI trajectory format uses 'obs' as the coordinate.  This is an example of an 'older' style of data formatting that doesn't really mesh well with modern software capabilities. \n",
    "\n",
    "* So, let's change that by using [.swap_dims](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.swap_dims.html) to change the coordinate from `obs` to `time`\n",
    "* Another thing, `latitude` and `longitude` are just long and annoying, lets [.rename](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.rename.html) them to `lat` and `lon`\n",
    "\n",
    "* Finally, the first and last part of the cruise the USV is being towed, so let's only include data from `2018-04-12T02` to `2018-06-10T18`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv2 = ds_usv.isel(trajectory=0)\n",
    "\n",
    "ds_usv2 = ds_usv2.swap_dims({'obs':'time'})\n",
    "\n",
    "ds_usv2 = ds_usv2.rename({'longitude':'lon','latitude':'lat'})\n",
    "\n",
    "ds_usv_subset = ds_usv2.sel(time=slice('2018-04-12T02','2018-06-10T18')) \n",
    "\n",
    "start_time=pd.to_datetime(str(ds_usv2.time.min().data)).strftime('%Y-%m-%dT%H:%m:%SZ') \n",
    "end_time=pd.to_datetime(str(ds_usv2.time.max().data)).strftime('%Y-%m-%dT%H:%m:%SZ') \n",
    "\n",
    "print('start: ',start_time,'end: ',end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's open 2 months of 0.2 km AVHRR OI SST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xarray`can open multiple files at once using string pattern matching.  \n",
    "  \n",
    "  In this case we open all the files that match our `filestr`, i.e. all the files for the 2080s. \n",
    "  \n",
    "  Each of these files (compressed) is approximately 800 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'PODAAC-GHCMC-4FM03'  #CMC SST looked up on podaac website\n",
    "#dataset_id = 'PODAAC-GHGMR-4FJ04'  #MUR SST looked up on podaac website\n",
    "\n",
    "gresult = p.granule_search(dataset_id=dataset_id,\n",
    "                           start_time=start_time,\n",
    "                           end_time=end_time,\n",
    "                           items_per_page='100')\n",
    "\n",
    "urls = putil.PodaacUtils.mine_opendap_urls_from_granule_search(gresult)\n",
    "\n",
    "urls = [w[:-5] for w in urls]  #remove html from urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see what one day looks like\n",
    "`xr.open_dataset(urls[0])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a day using cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.Orthographic(-80, 35))  #this sets projection for plot\n",
    "\n",
    "ds_sst.analysed_sst.plot(ax=ax, transform=ccrs.PlateCarree())  #this tells projection of data\n",
    "\n",
    "#ax.coastlines(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mask out land values\n",
    "\n",
    "`ds_sst.where(ds_sst.mask<=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst_masked = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the colormap, colorscale, and add land\n",
    "\n",
    "`cmap='jet'`\n",
    "`vmin=-1,vmax=34`\n",
    "`ax.stock_img();`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now open multiple files (lazy) using [.open_mfdataset](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html#xarray.open_mfdataset)\n",
    "\n",
    "* use the option `coords = 'minimal'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst = xr.open_mfdataset('./data/avhrr_oi/*.nc',coords='minimal').isel(zlev=0)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again with the 0-360 vs -180-180.  Change it up below!\n",
    "\n",
    "* `ds_sst.coords['lon'] = (np.mod(ds_sst.coords['lon'] + 180,360) - 180)`\n",
    "\n",
    "* `ds_sst = ds_sst.sortby(ds_sst.lon)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst.coords['lon'] = (np.mod(ds_sst.coords['lon'] + 180,360) - 180)\n",
    "ds_sst = ds_sst.sortby(ds_sst.lon)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xarray` even puts them in the right order for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is all this data uncompressed? Will it fit into memory?\n",
    "Use `.nbytes` / 1e9  to convert it into gigabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size: :', ds_sst.nbytes / 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xarray interpolation won't run on chunked dimensions.  \n",
    "1. First let's subset the data to make it smaller to deal with by using the cruise lat/lons\n",
    "    * Find the max/min of the lat/lon using `.lon.min().data`\n",
    "\n",
    "1. Now load the data into memory (de-Dask-ify) it using `.load()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 from above\n",
    "lon_min,lon_max = ds_usv_subset.lon.min().data,ds_usv_subset.lon.max().data\n",
    "lat_min,lat_max = ds_usv_subset.lat.min().data,ds_usv_subset.lat.max().data\n",
    "\n",
    "subset = ds_sst.sel(lon=slice(lon_min,lon_max),\n",
    "                    lat=slice(lat_min,lat_max))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size: :', subset.nbytes / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 from above\n",
    "subset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate USV data with MUR data\n",
    "There are different options when you interpolate.  First, let's just do a linear interpolation using [.interp()](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.interp.html#xarray.Dataset.interp)\n",
    "\n",
    "`Dataset.interp(coords=None, method='linear', assume_sorted=False, kwargs={}, **coords_kwargs))`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated = subset.interp(lat=ds_usv_subset.lat,\n",
    "                              lon=ds_usv_subset.lon,\n",
    "                              time=ds_usv_subset.time,\n",
    "                              method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate USV data with MUR data\n",
    "There are different options when you interpolate.  First, let's just do a nearest point rather than interpolate the data\n",
    "`method = 'nearest'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_nearest = subset.interp(lat=ds_usv_subset.lat,\n",
    "                                      lon=ds_usv_subset.lon,\n",
    "                                      time=ds_usv_subset.time,\n",
    "                                      method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, calculate the different in SSTs and print the [.mean()](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.mean.html#xarray.DataArray.mean) and [.std()](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.std.html#xarray.DataArray.std)\n",
    "For the satellite data we need to use `sst` and for the USV data we need to use `TEMP_CTD_MEAN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = ds_collocated_nearest.sst-ds_usv_subset.TEMP_CTD_MEAN\n",
    "\n",
    "print('mean difference = ',dif.mean().data)\n",
    "print('STD = ',dif.std().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xarray can do more!\n",
    "\n",
    "* concatentaion\n",
    "* open network located files with openDAP\n",
    "* import and export Pandas DataFrames\n",
    "* .nc dump to \n",
    "* groupby_bins\n",
    "* resampling and reduction\n",
    "\n",
    "For more details, read this blog post: http://continuum.io/blog/xray-dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_collocated_nearest.to_netcdf('./data/new file.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where can I find more info?\n",
    "\n",
    "### For more information about xarray\n",
    "\n",
    "- Read the [online documentation](http://xarray.pydata.org/)\n",
    "- Ask questions on [StackOverflow](http://stackoverflow.com/questions/tagged/python-xarray)\n",
    "- View the source code and file bug reports on [GitHub](http://github.com/pydata/xarray/)\n",
    "\n",
    "### For more doing data analysis with Python:\n",
    "\n",
    "- Thomas Wiecki, [A modern guide to getting started with Data Science and Python](http://twiecki.github.io/blog/2014/11/18/python-for-data-science/)\n",
    "- Wes McKinney, [Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do) (book)\n",
    "\n",
    "### Packages building on xarray for the geophysical sciences\n",
    "\n",
    "For analyzing GCM output:\n",
    "\n",
    "- [xgcm](https://github.com/xgcm/xgcm) by Ryan Abernathey\n",
    "- [oogcm](https://github.com/lesommer/oocgcm) by Julien Le Sommer\n",
    "- [MPAS xarray](https://github.com/pwolfram/mpas_xarray) by Phil Wolfram\n",
    "- [marc_analysis](https://github.com/darothen/marc_analysis) by Daniel Rothenberg\n",
    "\n",
    "Other tools:\n",
    "\n",
    "- [windspharm](https://github.com/ajdawson/windspharm): wind spherical harmonics by Andrew Dawson\n",
    "- [eofs](https://github.com/ajdawson/eofs): empirical orthogonal functions by Andrew Dawson\n",
    "- [infinite-diff](https://github.com/spencerahill/infinite-diff) by Spencer Hill \n",
    "- [aospy](https://github.com/spencerahill/aospy) by Spencer Hill and Spencer Clark\n",
    "- [regionmask](https://github.com/mathause/regionmask) by Mathias Hauser\n",
    "- [salem](https://github.com/fmaussion/salem) by Fabien Maussion\n",
    "\n",
    "Resources for teaching and learning xarray in geosciences:\n",
    "- [Fabien's teaching repo](https://github.com/fmaussion/teaching): courses that combine teaching climatology and xarray\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
