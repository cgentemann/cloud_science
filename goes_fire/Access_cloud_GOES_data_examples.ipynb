{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Accessing cloud satellite data\n",
    "\n",
    "- Funding: Interagency Implementation and Advanced Concepts Team [IMPACT](https://earthdata.nasa.gov/esds/impact) for the Earth Science Data Systems (ESDS) program and AWS Public Dataset Program\n",
    "  \n",
    "### Credits: Tutorial development\n",
    "* [Dr. Chelle Gentemann](mailto:gentemann@faralloninstitute.org) -  [Twitter](https://twitter.com/ChelleGentemann)   - Farallon Institute\n",
    "\n",
    "### Satellite data\n",
    "\n",
    "Satellite data come from either polar or low inclination orbits that circle the Earth or geostationary orbits that hold a location above a specific location on the Earth, near the Equator.  The type of data from satellites depends on both the orbit and the type of instrument.  There are definitions for the different satellite data types.\n",
    "- L1 - satellite observations, usually each observation has a lat/lon/time associated with it\n",
    "- L2 - derived geophysical retrievals (eg. SST), still with lat/lon/time\n",
    "- L3 - variables mapped to a uniform space/time grid\n",
    "- L4 - model output or analysis of data, may include multiple sensors\n",
    "\n",
    "There are several different formats that satellite data:  [netCDF](https://www.unidata.ucar.edu/software/netcdf/), [HDF5](https://www.hdfgroup.org/solutions/hdf5/), and [geoTIFF](https://earthdata.nasa.gov/esdis/eso/standards-and-references/geotiff).\n",
    "There are also som newer cloud optimized formats that data: [Zarr](https://zarr.readthedocs.io/en/stable/), Cloud Optimized GeoTIFF ([COG](https://www.cogeo.org/)).\n",
    "\n",
    "### Understand the data\n",
    "Data access can be easy and fast on the cloud, but to avoid potential issues down the road, spend time to understand the strengths and weaknesses of any data.  Any science results will need to address uncertainties in the data, so it is easier to spend time at the beginning to understand them and avoid data mis-application.\n",
    "\n",
    "#### This tutorial will focus on netCDF/HDF5/Zarr\n",
    "\n",
    "### Data proximate computing\n",
    "These are BIG datasets that you can analyze on the cloud without downloading the data.  \n",
    "You can run this on your phone, a Raspberry Pi, laptop, or desktop.   \n",
    "By using public cloud data, your science is reproducible and easily shared!\n",
    "\n",
    "The OHW Jupyter Hub is a m5.2xlarge instance shared by up to 4 users.  What does that mean?  [m5.2xlarge](https://aws.amazon.com/ec2/instance-types/m5/) has 8 vCPU and 32 GB memory.\n",
    "If there are 3 other users on your node, then the memory could be used up fast by large satellite datasets, so just be a little aware of that when you want to run something....\n",
    "\n",
    "### Here we will demonstrate some ways to access the different SST datasets on AWS:\n",
    "- AWS MUR sea surface temperatures  (L4)\n",
    "- AWS GOES sea surface temperatures  (L2)\n",
    "- AWS Himawari sea surface temperatures (L2 and L3)\n",
    "- ERA5 data (L4)\n",
    "\n",
    "### To run this notebook\n",
    "\n",
    "Code is in the cells that have <span style=\"color: blue;\">In [  ]:</span> to the left of the cell and have a colored background\n",
    "\n",
    "To run the code:\n",
    "- option 1) click anywhere in the cell, then hold `shift` down and press `Enter`\n",
    "- option 2) click on the Run button at the top of the page in the dashboard\n",
    "\n",
    "Remember:\n",
    "- to insert a new cell below press `Esc` then `b`\n",
    "- to delete a cell press `Esc` then `dd`\n",
    "\n",
    "### First start by importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some warning messages\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "#libraries\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import s3fs\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# make datasets display nicely\n",
    "xr.set_options(display_style=\"html\")  \n",
    "\n",
    "#magic fncts #put static images of your plot embedded in the notebook\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to find an AWS Public Dataset\n",
    "\n",
    "- click here: [AWS Public Dataset](https://aws.amazon.com/opendata/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click on `Find public available data on AWS` button\n",
    "- Search for GOES\n",
    "- Select [GOES-16 and GOES-17](https://registry.opendata.aws/noaa-goes/)\n",
    "![](./aws.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NETCDF GEO DATA\n",
    "\n",
    "## Zarr made it seem so easy to read and access cloud SST data! \n",
    "## It is always that easy, right?\n",
    "\n",
    "- When data is just dumped on the cloud, it is still useful, but can be challenging to use.  \n",
    "Which brings us to GOES SSTs.....\n",
    "\n",
    "- Info on the data is here -- GOES has a lot of different parameters and they are all stored in different files with names that are difficult to understand.  There are *80* different data products.  This link lists them all and explains the different directory names. [AWS info on GOES SST data](https://docs.opendata.aws/noaa-goes16/cics-readme.html).  \n",
    "\n",
    "- The GOES data is netCDF format.  There is a different for each of the 80 projects for year/day/hour.  A lot of files.  I find it really useful to 'browse' s3: buckets so that I can understand the directory and data structure.  [Explore S3 structure](https://noaa-goes16.s3.amazonaws.com/index.html).  The directory structure is `<Product>/<Year>/<Day of Year>/<Hour>/<Filename>`\n",
    "\n",
    "- In the code below we are going to create a function that searches for all files from a certain day, then creates the keys to opening them.  The files are so messy that opening a day takes a little while.  There are other ways you could write this function depending on what your analysis goals are, this is just one way to get some data in a reasonable amount of time. \n",
    "- This function uses \n",
    "- [`s3fs.S3FileSystem`](https://s3fs.readthedocs.io/en/latest/) which holds a connection with a s3 bucket and allows you to list files, etc.  \n",
    "- [`xr.open_mfdataset`](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html#xarray.open_mfdataset) opens a list of filenames and concatenates the data along the specified dimensions  \n",
    "\n",
    "Website [https://registry.opendata.aws/noaa-goes/](https://registry.opendata.aws/noaa-goes/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geo_data(sat,lyr,idyjl):\n",
    "    # arguments\n",
    "    # sat   goes-east,goes-west,himawari\n",
    "    # lyr   year\n",
    "    # idyjl day of year\n",
    "    \n",
    "    d = dt.datetime(lyr,1,1) + dt.timedelta(days=idyjl)\n",
    "    fs = s3fs.S3FileSystem(anon=True) #connect to s3 bucket!\n",
    "\n",
    "    #create strings for the year and julian day\n",
    "    imon,idym=d.month,d.day\n",
    "    syr,sjdy,smon,sdym = str(lyr).zfill(4),str(idyjl).zfill(3),str(imon).zfill(2),str(idym).zfill(2)\n",
    "    \n",
    "    #use glob to list all the files in the directory\n",
    "    if sat=='goes-east':\n",
    "        file_location,var = fs.glob('s3://noaa-goes16/ABI-L2-SSTF/'+syr+'/'+sjdy+'/*/*.nc'),'SST'\n",
    "    if sat=='goes-west':\n",
    "        file_location,var = fs.glob('s3://noaa-goes17/ABI-L2-SSTF/'+syr+'/'+sjdy+'/*/*.nc'),'SST'\n",
    "    if sat=='himawari':\n",
    "        file_location,var = fs.glob('s3://noaa-himawari8/AHI-L2-FLDK-SST/'+syr+'/'+smon+'/'+sdym+'/*/*L2P*.nc'),'sea_surface_temperature'\n",
    "    \n",
    "    #make a list of links to the file keys\n",
    "    if len(file_location)<1:\n",
    "        return file_ob\n",
    "    file_ob = [fs.open(file) for file in file_location]        #open connection to files\n",
    "    \n",
    "    #open all the day's data\n",
    "    ds = xr.open_mfdataset(file_ob,combine='nested',concat_dim='time') #note file is super messed up formatting\n",
    "    \n",
    "    #clean up coordinates which are a MESS in GOES\n",
    "    #rename one of the coordinates that doesn't match a dim & should\n",
    "    if not sat=='himawari':\n",
    "        ds = ds.rename({'t':'time'})\n",
    "        ds = ds.reset_coords()\n",
    "    else:\n",
    "        ds = ds.rename({'ni':'x','nj':'y'})\n",
    "    \n",
    "    #put in to Celsius\n",
    "    #ds[var] -= 273.15   #nice python shortcut to +- from itself a-=273.15 is the same as a=a-273.15\n",
    "    #ds[var].attrs['units'] = '$^\\circ$C'\n",
    "   \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a day of GOES-16  (East Coast) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lyr, idyjl = 2020, 210  #may 30, 2020\n",
    "\n",
    "ds = get_geo_data('goes-east',lyr,idyjl)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subset = ds.sel(x=slice(-0.01,0.07215601),y=slice(0.12,0.09))  #reduce to GS region\n",
    "\n",
    "masked = subset.SST.where(subset.DQF==0)\n",
    "\n",
    "masked.isel(time=14).plot(vmin=14+273.15,vmax=30+273.15,cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#mean_dy = masked.mean('time',skipna=True)   #here I want all possible values so skipna=True\n",
    "\n",
    "#mean_dy.plot(vmin=14+273.15,vmax=30+273.15,cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
