{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCMP Winds in a cloud-optimized-format for Pangeo\n",
    "\n",
    "The Cross-Calibrated Multi-Platform (CCMP) Ocean Surface Wind Vector Analyses is part of the NASA Making Earth System Data Records for Use in Research Environments (MEaSUREs) Program. MEaSUREs, develops consistent global- and continental-scale Earth System Data Records by supporting projects that produce data using proven algorithms and input.  If you use this data, please give [credit](https://podaac.jpl.nasa.gov/MEaSUREs-CCMP?sections=about).  For more information, please review the [documentation](https://podaac-tools.jpl.nasa.gov/drive/files/allData/ccmp/L2.5/docs/ccmp_users_guide.pdf). Please note that this data is not recommended for trend calculations.\n",
    "\n",
    "# Accessing cloud satellite data\n",
    "\n",
    "- CCMP zarr conversion funding: Interagency Implementation and Advanced Concepts Team [IMPACT](https://earthdata.nasa.gov/esds/impact) for the Earth Science Data Systems (ESDS) program and AWS Public Dataset Program\n",
    "  \n",
    "### Credits: Tutorial development\n",
    "* [Dr. Chelle Gentemann](mailto:gentemann@faralloninstitute.org) -  [Twitter](https://twitter.com/ChelleGentemann)   - Farallon Institute\n",
    "\n",
    "### Zarr data format\n",
    "\n",
    " [Zarr](https://zarr.readthedocs.io/en/stable/)\n",
    "\n",
    "### Data proximate computing\n",
    "These are BIG datasets that you can analyze on the cloud without downloading the data. You can run this on your phone, a Raspberry Pi, laptop, or desktop.   \n",
    "By using public cloud data, your science is reproducible and easily shared!\n",
    "\n",
    "### To run this notebook\n",
    "\n",
    "Code is in the cells that have <span style=\"color: blue;\">In [  ]:</span> to the left of the cell and have a colored background\n",
    "\n",
    "To run the code:\n",
    "- option 1) click anywhere in the cell, then hold `shift` down and press `Enter`\n",
    "- option 2) click on the Run button at the top of the page in the dashboard\n",
    "\n",
    "Remember:\n",
    "- to insert a new cell below press `Esc` then `b`\n",
    "- to delete a cell press `Esc` then `dd`\n",
    "\n",
    "### First start by importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libs for reading data\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xhistogram.xarray import histogram\n",
    "\n",
    "#lib for dask gateway\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a cluster, a group of computers that will work together.\n",
    "\n",
    "(A cluster is the key to big data analysis on on Cloud.)\n",
    "\n",
    "- This will set up a [dask kubernetes](https://docs.dask.org/en/latest/setup/kubernetes.html) cluster for your analysis and give you a path that you can paste into the top of the Dask dashboard to visualize parts of your cluster.  \n",
    "- You don't need to paste the link below into the Dask dashboard for this to work, but it will help you visualize progress.\n",
    "- Try 20 workers to start (during the tutorial) but you can increase to speed things up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = Gateway()\n",
    "cluster = gateway.new_cluster()\n",
    "cluster.adapt(minimum=1, maximum=75)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ☝️ Don’t forget to click the link above or copy it to the Dask dashboard ![images.png](attachment:images.png) on the left to view the scheduler dashboard! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataset\n",
    "\n",
    "Here we load the dataset from the zarr store. Note that this very large dataset (273 GB) initializes nearly instantly, and we can see the full list of variables and coordinates.\n",
    "\n",
    "### Examine Metadata\n",
    "\n",
    "For those unfamiliar with this dataset, the variable metadata is very helpful for understanding what the variables actually represent\n",
    "Printing the dataset will show you the dimensions, coordinates, and data variables with clickable icons at the end that show more metadata and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intake import open_catalog\n",
    "\n",
    "cat = open_catalog(\"https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/atmosphere.yaml\")\n",
    "\n",
    "ds = cat['nasa_ccmp_wind_vectors'].to_dask()\n",
    "\n",
    "ds['wspd']=np.sqrt(ds.uwnd**2+ds.vwnd**2)  #calculate wind speed\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a global image of the data on 7/28/2020\n",
    "\n",
    "``xarray`` makes plotting the data very easy.  A nice overview of plotting with xarray is [here](http://xarray.pydata.org/en/stable/plotting.html).  Details on [.plot](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.plot.html#xarray.DataArray.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ds.sel(time='2020-07-04T00')\n",
    "\n",
    "day.nobs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a land/ocean/ice mask to show where there is actually data\n",
    "\n",
    "### Three different ways to mask the data\n",
    "1. A daily mask that removes data with sea ice and land\n",
    "- sum over time for nobs (number of observations) variable\n",
    "- average over a month so that land and monthly sea ice are masked out\n",
    "2. A mask that removes all data that over land or where there is 'permanent' sea ice\n",
    "- find when nobs is > 0\n",
    "3. A climatology mask that removes all data that over land or where there has ever been sea ice\n",
    "- sum over time for nobs (number of observations) variable\n",
    "- average over a month so that land and monthly sea ice are masked out\n",
    "\n",
    "# Apply the mask \n",
    "- over land, CCMP is ERA5 data\n",
    "- for many ocean applications a land / sea ice mask is needed\n",
    "- below are some different mask options that use the CCMP data to generate a mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(ds,type):\n",
    "    if type=='daily': #daily mask removes sea ice and land\n",
    "        mask_obs = ds.nobs.rolling(time=180,center=True).max('time')  #4 per day 30 days = 180 rolling window\n",
    "        cutoff = 0\n",
    "    if type=='land':  # land mask only (includes data over sea ice)\n",
    "        mask_obs = ds.nobs.sum({'time'},keep_attrs=True)  #this will give you a LAND mask\n",
    "        cutoff = 0\n",
    "    if type=='climatology':  #climatology mask removes max sea ice extent and land\n",
    "        mask_obs = ds.nobs.rolling(time=180,center=True).max('time')  #4 per day 30 days = 180 rolling window\n",
    "        mask_obs = mask_obs.sum({'time'},keep_attrs=True)\n",
    "        cutoff = 125000\n",
    "    dy_mask = mask_obs>cutoff\n",
    "    dy_mask = dy_mask.compute() #computing the mask speeds up subsequent operations\n",
    "    masked = ds.where(dy_mask)\n",
    "    return masked,dy_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print what the different masks look like\n",
    "- This next cell block will take a while as the masks are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subset=ds.isel(time=slice(500,3500))\n",
    "masked1,dy_mask = mask_data(subset,'daily')\n",
    "masked2,land_mask = mask_data(subset,'land')\n",
    "masked3,clim_mask = mask_data(ds,'climatology')\n",
    "fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "masked1.wspd.isel(time=500).plot(ax=ax[0])\n",
    "masked2.wspd.isel(time=500).plot(ax=ax[1])\n",
    "masked3.wspd.isel(time=1000).plot(ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this we will use the climatology mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide which mask to use 1=land/ice, 2=land, 3=climatology\n",
    "masked,mask_obs = mask_data(ds,'climatology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_obs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "masked.wspd[100,:,:].plot(ax=ax[0])\n",
    "masked.wspd[-100,:,:].plot(ax=ax[1])\n",
    "masked.wspd[5000,:,:].plot(ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a weighted global mean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://gallery.pangeo.io/repos/pangeo-gallery/cmip6/global_mean_surface_temp.html\n",
    "def global_mean(ds):\n",
    "    lat = ds.latitude\n",
    "    weight = np.cos(np.deg2rad(lat))\n",
    "    weight /= weight.mean()\n",
    "    other_dims = set(ds.dims) - {'time'}\n",
    "    return (ds * weight).mean(other_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate the global mean\n",
    "- I wish I didn't have to have these loops.  Programatically, it would be much cleaner to just do: \n",
    "```python\n",
    "#glb_mn = global_mean(masked)\n",
    "#glb_mn = glb_mn.compute()\n",
    "#print(glb_mn) \n",
    "```\n",
    "- but this code doesn't run, it kills my kernel (memory?) every time I try\n",
    "- for some reason if I run it year by year it runs fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,x=[],[]\n",
    "for lyr in range(1988,2020):\n",
    "    subset = masked.sel(time=str(lyr))\n",
    "    m1 = global_mean(subset)\n",
    "    m1 = m1.mean()\n",
    "    m1_computed = m1.compute()\n",
    "    m.append(m1_computed)\n",
    "    x.append(lyr)\n",
    "    print(lyr)\n",
    "mn_yr = xr.concat(m, dim='time')\n",
    "mn_yr['time']=np.arange(1988,2020)\n",
    "glb_mn = np.mean(mn_yr)\n",
    "print(glb_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "-glb_mean = 1988 - 2019 41 years\n",
    "-    nobs     1.296\n",
    "-    uwnd     -0.4763\n",
    "-    vwnd     0.2749\n",
    "-    wspd     8.558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "mn_yr.wspd.plot()\n",
    "#plt.legend(fontsize=8)\n",
    "plt.xlim(1988,2020)\n",
    "#plt.ylim()\n",
    "plt.ylabel('CCMPv2 Wind Speed (m s$^{-1}$)',fontsize=18)\n",
    "plt.xlabel('Year',fontsize=18)\n",
    "#plt.text(10,0.011,'CCMPv2 1988-2019 ',fontsize=18)\n",
    "plt.text(2005,8.5,'Global mean = 8.6 m s$^{-1}$',fontsize=16)\n",
    "#plt.text(10,0.009,'67% of winds are > 6 m s$^{-1}$',fontsize=16)\n",
    "plt.savefig('./../../figures/ccmp_ts_mean.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global Histogram figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,30,.1)\n",
    "h,x=[],[]\n",
    "for lyr in range(1988,2020):\n",
    "    subset = masked.wspd.sel(time=str(lyr))\n",
    "    h1 = histogram(subset, bins=[bins])\n",
    "    h1 = h1.compute()\n",
    "    print('start',lyr)\n",
    "    h.append(h1)\n",
    "    x.append(lyr)\n",
    "    hh = xr.concat(h, dim='time')\n",
    "    hh.to_netcdf('./../../data/ccmp/ccmp_annual_hist_20210507a.nc')\n",
    "    print('end',lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh=xr.open_dataset('./../../data/ccmp/ccmp_annual_hist_20210507.nc')\n",
    "hh1=xr.open_dataset('./../../data/ccmp/ccmp_annual_hist_20210507a.nc')\n",
    "#hh1.assign_coords['time']=hh1.time+27\n",
    "hh=xr.concat([hh,hh1],dim='time')\n",
    "hh['time']=np.arange(1988,2020)\n",
    "hh.to_netcdf('./../../data/ccmp/ccmp_annual_hist_20210507_final.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = xr.open_dataset('./../../data/ccmp/ccmp_annual_hist_20210507_final.nc')\n",
    "hhall = hh.histogram_wspd.sum('time')\n",
    "hhall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr = hh.histogram_wspd[0,:].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('percentage of winds =< 2 m/s',hhall[0:21].sum()/hhall.sum())\n",
    "print('percentage of winds =< 6 m/s',hhall[0:60].sum()/hhall.sum())\n",
    "print('percentage of winds > 6 m/s',hhall[60:].sum()/hhall.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh2=hh\n",
    "x=hh.time\n",
    "plt.rcParams['figure.figsize'] = (8,8)\n",
    "for iyr in range(32):\n",
    "    plt.plot(hh.wspd_bin,hh2.histogram_wspd[iyr,:]/hh2.histogram_wspd[iyr,:].sum(),label=str(x[iyr].data))\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim(-0,32)\n",
    "plt.ylim(0,.013)\n",
    "plt.xlabel('CCMP Wind Speed (m s$^{-1}$)',fontsize=18)\n",
    "plt.ylabel('PDF (s m$^{-1}$)',fontsize=18)\n",
    "plt.text(11,0.011,'CCMPv2 1988-2019 ',fontsize=18)\n",
    "plt.text(11,0.010,'Global mean = 8.6 m s$^{-1}$',fontsize=16)\n",
    "plt.text(11,0.009,'68% of winds are > 6 m s$^{-1}$',fontsize=16)\n",
    "plt.savefig('./../../figures/ccmp_annual_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8,8)\n",
    "hhall = hh2.sum('time')\n",
    "plt.plot(hh.wspd_bin,hhall.histogram_wspd/hhall.histogram_wspd.sum(),linewidth=5)\n",
    "plt.xlim(-0,30)\n",
    "plt.ylim(0,.012)\n",
    "plt.xlabel('CCMP Wind Speed (m s$^{-1}$)',fontsize=18)\n",
    "plt.ylabel('PDF (s m$^{-1}$)',fontsize=18)\n",
    "plt.text(10,0.011,'CCMPv2 1988-2019 ',fontsize=18)\n",
    "plt.text(10,0.010,'Global mean = 8.6 m s$^{-1}$',fontsize=18)\n",
    "plt.text(10,0.009,'68% of winds are > 6 m s$^{-1}$',fontsize=18)\n",
    "plt.savefig('./../../figures/ccmp_all_hist2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,30,.1)\n",
    "h,x=[],[]\n",
    "for lyr in range(1988,2020):\n",
    "    subset = masked.wspd.sel(time=str(lyr))\n",
    "    h1 = histogram(subset, bins=[bins])\n",
    "    h1 = h1.compute()\n",
    "    print('start',lyr)\n",
    "    h.append(h1)\n",
    "    x.append(lyr)\n",
    "    hh = xr.concat(h, dim='time')\n",
    "    hh.to_netcdf('./../../data/ccmp/ccmp_annual_hist_20210507a.nc')\n",
    "    print('end',lyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maps of wind speed distributions for c.donlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calc % winds\n",
    "#a spatial map showing a climatology of roughness.  \n",
    "#Ideally in 3 panels - (a) at Hs=<2, (b) at Hs=mean wind speed (c) Hs> 10 \n",
    "wnd = ds.wspd.where(ds.wspd<=2)\n",
    "f2 = (wnd/wnd).sum({'time'})/len(wnd.time)*100  # percent less than or equal to 2 m/s\n",
    "wnd = ds.wspd.where((ds.wspd>=8)&(ds.wspd<=9))\n",
    "f8 = (wnd/wnd).sum({'time'})/len(wnd.time)*100  # percent 8-9 m/s\n",
    "wnd = ds.wspd.where(ds.wspd>10)\n",
    "f10 = (wnd/wnd).sum({'time'})/len(wnd.time)*100  # percent >= 10 m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f2 = f2.compute()\n",
    "f8 = f8.compute()\n",
    "f10 = f10.compute()\n",
    "ff = xr.concat([f2,f8,f10],dim='frac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15.0,8.0)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "fg = ff.plot(aspect=1, size=10, vmin=0, vmax=100,\n",
    "    col=\"frac\",\n",
    "    transform=ccrs.PlateCarree(),  # remember to provide this!\n",
    "    subplot_kws={\n",
    "        \"projection\": ccrs.PlateCarree()\n",
    "    },\n",
    "    cbar_kwargs={\"label\":'Percent',\"orientation\": \"horizontal\", \"shrink\": 0.8, \"aspect\": 40},\n",
    "    robust=True,\n",
    ")\n",
    "tstr = ['< 2 m/s','8-9 m/s','> 10 m/s']\n",
    "for i, ax in enumerate(fg.axes.flat):\n",
    "    ax.set_title(tstr[i]) \n",
    "fg.map(lambda: plt.gca().coastlines())\n",
    "fig_fname = '../../figures/map_global_wind_distributions.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff2 = ff.where(mask_obs>0)\n",
    "fg = ff2.plot(aspect=1, size=10, vmin=0, vmax=100,\n",
    "    col=\"frac\",\n",
    "    transform=ccrs.PlateCarree(),  # remember to provide this!\n",
    "    subplot_kws={\n",
    "        \"projection\": ccrs.PlateCarree()\n",
    "    },\n",
    "    cbar_kwargs={\"label\":'Percent',\"orientation\": \"horizontal\", \"shrink\": 0.8, \"aspect\": 40},\n",
    "    robust=True,\n",
    ")\n",
    "tstr = ['< 2 m/s','8-9 m/s','> 10 m/s']\n",
    "for i, ax in enumerate(fg.axes.flat):\n",
    "    ax.set_title(tstr[i]) \n",
    "fg.map(lambda: plt.gca().coastlines())\n",
    "fig_fname = '../../figures/map_ocean_wind_distributions.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "plt.rcParams['figure.figsize'] = (18.0,5.0)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "ax = plt.subplot(131,projection=ccrs.PlateCarree())\n",
    "cs=f2.plot(ax=ax,vmin=0,vmax=100,cbar_kwargs={'shrink':.35,'label': 'Wind < 2 m/s'})\n",
    "ax.coastlines()\n",
    "ax = plt.subplot(132,projection=ccrs.PlateCarree())\n",
    "cs=f8.plot(ax=ax,vmin=0,vmax=100,cbar_kwargs={'shrink':.35,'label': 'Wind 8-9 m/s'})\n",
    "ax.coastlines()\n",
    "ax = plt.subplot(133,projection=ccrs.PlateCarree())\n",
    "cs=f10.plot(ax=ax,vmin=0,vmax=100,cbar_kwargs={'shrink':.35,'label': 'Wind > 10 m/s'})\n",
    "ax.coastlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weibull distributions TESTING STILL\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out weibull at one point with data and without data\n",
    "import scipy.stats as stats\n",
    "data = ds.wspd[:,0,400].load()\n",
    "params = stats.exponweib.fit(data, floc=0, f0=1)\n",
    "shape = params[1]\n",
    "scale = params[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,bins,hist = plt.hist(data,bins=51,range=(0,25),density=True)\n",
    "center = (bins[:-1] + bins[1:]) / 2.\n",
    "# Using all params and the stats function\n",
    "params = stats.exponweib.fit(data, floc=0, f0=1)\n",
    "plt.plot(center,stats.exponweib.pdf(center,*params),lw=4,label='scipy exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = stats.exponweib.fit(ds.wspd, floc=0, f0=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.to_netcdf('./../../data/weib.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://gist.github.com/luke-gregor/4bb5c483b2d111e52413b260311fbe43\n",
    "def dataset_encoding(xds):\n",
    "    cols = ['source', 'original_shape', 'dtype', 'zlib', 'complevel', 'chunksizes']\n",
    "    info = pd.DataFrame(columns=cols, index=xds.data_vars)\n",
    "    for row in info.index:\n",
    "        var_encoding = xds[row].encoding\n",
    "        for col in info.keys():\n",
    "            info.ix[row, col] = var_encoding.pop(col, '')\n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "def xarray_trend(xarr):    \n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    # getting shapes\n",
    "    \n",
    "    m = np.prod(xarr.shape[1:]).squeeze()\n",
    "    n = xarr.shape[0]\n",
    "    \n",
    "    # creating x and y variables for linear regression\n",
    "    #x = xarr.time.to_pandas().index.to_julian_date().values[:, None]\n",
    "    y = xarr.to_masked_array().reshape(n, -1)\n",
    "    \n",
    "    # ############################ #\n",
    "    # LINEAR REGRESSION DONE BELOW #\n",
    "    params = stats.exponweib.fit(y, floc=0, f0=1)\n",
    "    shape = params[1]\n",
    "    scale = params[3]\n",
    "  \n",
    "    # preparing outputs\n",
    "    out = xarr[:2].mean('time')\n",
    "    # first create variable for slope and adjust meta\n",
    "    xarr_slope = out.copy()\n",
    "    xarr_slope.name += '_shape'\n",
    "    xarr_slope.attrs['units'] = 'none'\n",
    "    xarr_slope.values = shape.reshape(xarr.shape[1:])\n",
    "    # do the same for the p value\n",
    "    xarr_p = out.copy()\n",
    "    xarr_p.name += '_scale'\n",
    "    xarr_p.attrs['info'] = \"none\"\n",
    "    xarr_p.values = p.reshape(xarr.shape[1:])\n",
    "    # join these variables\n",
    "    xarr_out = xarr_slope.to_dataset(name='shape')\n",
    "    xarr_out['scale'] = xarr_p\n",
    "\n",
    "    return xarr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_slope2=[]\n",
    "for inc in range(0,1):\n",
    "    mlon=inc*5\n",
    "    mlon2 = (inc+1)*5-1\n",
    "    subset = ds.wspd.sel(longitude=slice(mlon,mlon2),latitude=slice(-78,-68)).load()\n",
    "    sst_slope = xarray_trend(subset)\n",
    "    sst_slope2.append(sst_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "# getting shapes\n",
    "\n",
    "xarr = subset\n",
    "\n",
    "m = np.prod(xarr.shape[1:]).squeeze()\n",
    "n = xarr.shape[0]\n",
    "\n",
    "# creating x and y variables for linear regression\n",
    "#x = xarr.time.to_pandas().index.to_julian_date().values[:, None]\n",
    "y = xarr.to_masked_array().reshape(n, -1)\n",
    "\n",
    "# ############################ #\n",
    "# LINEAR REGRESSION DONE BELOW #\n",
    "params = stats.exponweib.fit(y, floc=0, f0=1)\n",
    "shape = params[1]\n",
    "scale = params[3]\n",
    "\n",
    "# preparing outputs\n",
    "out = xarr[:2].mean('time')\n",
    "# first create variable for slope and adjust meta\n",
    "xarr_slope = out.copy()\n",
    "xarr_slope.name += '_shape'\n",
    "xarr_slope.attrs['units'] = 'none'\n",
    "xarr_slope.values = shape.reshape(xarr.shape[1:])\n",
    "# do the same for the p value\n",
    "xarr_p = out.copy()\n",
    "xarr_p.name += '_scale'\n",
    "xarr_p.attrs['info'] = \"none\"\n",
    "xarr_p.values = p.reshape(xarr.shape[1:])\n",
    "# join these variables\n",
    "xarr_out = xarr_slope.to_dataset(name='shape')\n",
    "xarr_out['scale'] = xarr_p\n",
    "\n",
    "return xarr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_slope2=[]\n",
    "for inc in range(0,35):\n",
    "    mlon=inc*10\n",
    "    mlon2 = (inc+1)*10-1\n",
    "    subset = ds.wspd.sel(longitude=slice(mlon,mlon2))\n",
    "    sst_slope = xarray_trend(subset)\n",
    "    sst_slope2.append(sst_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
