{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greatest-nerve",
   "metadata": {},
   "source": [
    "# AWS ERA5 data\n",
    "\n",
    "- remaking ERA5 from AWS zarr store\n",
    "- not working zarr to zarr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook based on zflemings:https://nbviewer.jupyter.org/github/zflamig/dask-era5/blob/main/notebook/era5_fargate_dask.ipynb\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import dask\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "xr.set_options(display_style=\"html\")  #display dataset nicely \n",
    "\n",
    "from fsspec_reference_maker.hdf import SingleHdf5ToZarr \n",
    "from fsspec_reference_maker.combine import MultiZarrToZarr\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "import s3fs\n",
    "import requests\n",
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from json import dumps\n",
    "from io import StringIO\n",
    "from os.path import dirname, join\n",
    "import netrc\n",
    "import dask.bag as db\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import fsspec\n",
    "import ujson   # fast json\n",
    "from fsspec_reference_maker.hdf import SingleHdf5ToZarr \n",
    "from fsspec_reference_maker.combine import MultiZarrToZarr\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import hvplot.xarray\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c0e0c-a469-472a-9f1e-331bc0fc67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = 's3://esip-qhub/ecmwf/era5/jsons_all/'\n",
    "json_consolidated_dir = 's3://esip-qhub-public/ecmwf/era5/'\n",
    "json_out1 = 'era5_consolidated.json'\n",
    "json_out = './../data/era5_consolidated_tem.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f508d4-9474-46d1-a586-231443e1dc5e",
   "metadata": {},
   "source": [
    "- open up cluster in dashboard and connect directly once you know IP\n",
    "- not sure why but gateway didn't work for this hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import ebdpy as ebd\n",
    "\n",
    "ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "profile = 'esip-qhub'\n",
    "region = 'us-west-2'\n",
    "endpoint = f's3.{region}.amazonaws.com'\n",
    "ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "worker_max = 30\n",
    "client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "                                      region=region, use_existing_cluster=True,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                      environment='pangeo', worker_profile='Medium Worker', \n",
    "                                      propagate_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cb801-b208-4563-b943-5e24f2ed2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of variables\n",
    "s3 = s3fs.S3FileSystem(anon=True, client_kwargs={'region_name':'us-east-1'})\n",
    "data_var = s3.ls('era5-pds/zarr/2021/04/data/')\n",
    "ddvar = []\n",
    "for dvar in data_var:\n",
    "    print(dvar[27:-5])\n",
    "    ddvar.append(dvar[27:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f388296-a6e4-4955-81d0-0ee389588a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var = ddvar[0]\n",
    "#fs = begin_s3_direct_access()\n",
    "fs = fsspec.filesystem('s3', anon=False)  \n",
    "flist = []\n",
    "for lyr in range(1979,2022):\n",
    "    for imon in range(0,12):\n",
    "        file = \"era5-pds/zarr/\"+str(lyr)+\"/\"+str(imon+1).zfill(2)+\"/data/\"+str(var)+\".zarr\"    \n",
    "        flist.append(file)\n",
    "print('total number of individual zarr files:',len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6c71f-8252-4766-8c08-3fdd49f52839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "urls = [\"s3://\" + f for f in flist]\n",
    "so = dict(mode='rb', anon=True, default_fill_cache=False, default_cache_type='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b1324-3016-4a05-bbef-7bd8ed1402dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs2 = fsspec.filesystem('s3', anon=False)  \n",
    "#If the directory exists, remove it (and all the files)\n",
    "try:\n",
    "    fs2.rm(json_dir, recursive=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa5b14-b713-4562-9aa0-6c9921acabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(u):\n",
    "    with fs.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        p = u.split('/')\n",
    "        date = p[4]+p[5] #p[3]\n",
    "        fname = p[7] #p[5]\n",
    "        outf = f'{json_dir}{date}.{fname}.json'\n",
    "        print(outf)\n",
    "        with fs2.open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e673d1-260a-4ca2-bdae-c3cc11d0619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = dask.compute(*[dask.delayed(gen_json)(u) for u in urls], retries=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bf1b6-3e51-48c6-b7d3-4cb6717ea7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = json_consolidated_dir + json_out1\n",
    "fs2.put_file(lpath=json_out, rpath=rpath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6cdb2-3298-4b6d-9010-02189e28fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the directory exists, remove it (and all the files)\n",
    "\n",
    "fs2.ls(json_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab408a0f-406e-434b-80cd-61d0636fd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = urls[0]\n",
    "p = u.split('/')\n",
    "date = p[4]+p[5] #p[3]\n",
    "fname = p[7] #p[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b81e98-ae9f-42aa-9e1f-fa6458e4ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9f15a-4761-43d0-8fe0-f455ed8cd235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_accum_var_dims(ds, var):\n",
    "    # Some varibles like precip have extra time bounds varibles, we drop them here to allow merging with other variables\n",
    "    \n",
    "    # Select variable of interest (drops dims that are not linked to current variable)\n",
    "    ds = ds[[var]]  \n",
    "\n",
    "    if var in ['air_temperature_at_2_metres',\n",
    "               'dew_point_temperature_at_2_metres',\n",
    "               'air_pressure_at_mean_sea_level',\n",
    "               'northward_wind_at_10_metres',\n",
    "               'eastward_wind_at_10_metres',\n",
    "               'eastward_wind_at_100_metres',\n",
    "               'northward_wind_at_100_metres',\n",
    "              'lwe_thickness_of_surface_snow_amount',\n",
    "              'sea_surface_temperature',\n",
    "              'surface_air_pressure',\n",
    "              'snow_density']:\n",
    "        \n",
    "        ds = ds.rename({'time0':'time','lat':'latitude','lon':'longitude'})\n",
    "        \n",
    "    elif var in ['precipitation_amount_1hour_Accumulation',\n",
    "                 'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation',\n",
    "                 'air_temperature_at_2_metres_1hour_Maximum',\n",
    "                 'air_temperature_at_2_metres_1hour_Minimum']:\n",
    "        \n",
    "        ds = ds.rename({'time1':'time','lat':'latitude','lon':'longitude'})\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning, Haven't seen {var} varible yet! Time renaming might not work.\".format(var=var))\n",
    "        \n",
    "    return ds\n",
    "\n",
    "@dask.delayed\n",
    "def s3open(path):\n",
    "    fs = s3fs.S3FileSystem(anon=True, default_fill_cache=False, \n",
    "                           config_kwargs = {'max_pool_connections': 20})\n",
    "    return s3fs.S3Map(path, s3=fs)\n",
    "\n",
    "\n",
    "def get_files(start_year, end_year):\n",
    "    file_pattern = 'era5-pds/zarr/{year}/{month}/data/{var}.zarr/'    \n",
    "    years = list(np.arange(start_year, end_year+1, 1))\n",
    "    months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "    \n",
    "\n",
    "\n",
    "def open_era5_range(start_year, end_year, variables):\n",
    "    ''' Opens ERA5 monthly Zarr files in S3, given a start and end year (all months loaded) and a list of variables'''\n",
    "    \n",
    "    \n",
    "    file_pattern = 'era5-pds/zarr/{year}/{month}/data/{var}.zarr/'\n",
    "    \n",
    "    years = list(np.arange(start_year, end_year+1, 1))\n",
    "    months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "    \n",
    "    l = []\n",
    "    for var in variables:\n",
    "        print('opening',var)\n",
    "        \n",
    "        # Get files\n",
    "        files_mapper = [s3open(file_pattern.format(year=year, month=month, var=var)) for year in years for month in months]\n",
    "        \n",
    "        # Look up correct time dimension by variable name\n",
    "        \n",
    "        if var in ['precipitation_amount_1hour_Accumulation',\n",
    "                 'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation',\n",
    "                 'air_temperature_at_2_metres_1hour_Maximum',\n",
    "                 'air_temperature_at_2_metres_1hour_Minimum']:\n",
    "            concat_dim='time1'\n",
    "        else:\n",
    "            concat_dim='time0'\n",
    "            \n",
    "        # Lazy load\n",
    "        ds = xr.open_mfdataset(files_mapper, engine='zarr', \n",
    "                               concat_dim=concat_dim, combine='nested', \n",
    "                               coords='minimal', compat='override', parallel=True)\n",
    "        \n",
    "        # Fix dimension names\n",
    "        ds = fix_accum_var_dims(ds, var)\n",
    "        l.append(ds)\n",
    "        \n",
    "    ds_out = xr.merge(l)\n",
    "    \n",
    "    return ds_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set AWS region to access ERA5 data\n",
    "s3 = s3fs.S3FileSystem(anon=True, client_kwargs={'region_name':'us-east-1'})\n",
    "data_var = s3.ls('era5-pds/zarr/2021/04/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9260d-9e36-4fba-873a-c184d35cff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddvar = []\n",
    "for dvar in data_var:\n",
    "    print(dvar[27:-5])\n",
    "    ddvar.append(dvar[27:-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669aed5-1a66-437e-8253-d6dcf1b1083b",
   "metadata": {},
   "source": [
    "- it took me a while to get this to run\n",
    "- the cluster kept crashing until I just continued breaking it down into smaller and smaller bits\n",
    "- subseting to year/month was finally the trick with ~30 workers and 200GB memory being used to find the mean for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@dask.delayed\n",
    "def downsample(ds):\n",
    "    ds_month  = ds.resample(time='1M').mean(keep_attrs=True)\n",
    "    ds_month_deg = ds_month.coarsen(latitude=8,longitude=8,boundary=\"trim\").mean(keep_attrs=True)\n",
    "    return ds_month_deg\n",
    "\n",
    "for idvar,dvar in enumerate(data_var):\n",
    "    tem = []\n",
    "    var = dvar[27:-5]\n",
    "    fout = './../../data/era5/era5_monthly_2deg'+var+'.nc'\n",
    "    if os.path.exists(fout):\n",
    "        continue\n",
    "    for lyr in range(1979,2021):\n",
    "        #print(lyr,dvar)        \n",
    "        #if var == 'air_pressure_at_mean_sea_level':\n",
    "        #    continue\n",
    "        print(lyr,var)\n",
    "        ds = open_era5_range(lyr,lyr, [var])\n",
    "        mn = []\n",
    "        tt= []\n",
    "        for i in range(12):\n",
    "            ds_month  = ds.sel(time=str(lyr)+'-'+str(i+1).zfill(2)).mean('time',keep_attrs=True)\n",
    "            ds_month = ds_month.assign_coords({'time':ds.sel(time=str(lyr)+'-'+str(i+1).zfill(2)).time.mean().data})\n",
    "            ds_month_deg = ds_month.coarsen(latitude=8,longitude=8,boundary=\"trim\").mean(keep_attrs=True)\n",
    "            ds_month_deg = ds_month_deg.load()\n",
    "            mn.append(ds_month_deg)  \n",
    "        mn = xr.concat(mn,dim='time')\n",
    "        tem.append(mn)\n",
    "        tem2 = xr.concat(tem,dim='time')\n",
    "        tem2 = tem2.sortby(tem2.latitude)\n",
    "        #tem2.to_zarr('./../../data/era5/era5_monthly_2deg'+var+'_1990.zarr')\n",
    "        tem2.to_netcdf('./../../data/era5/era5_monthly_2deg'+var+'.nc')\n",
    "        print('wrote:', lyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39259c57-51e4-4124-b1b3-191d742d9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idvar,dvar in enumerate(data_var):\n",
    "    tem = []\n",
    "    var = dvar[27:-5]\n",
    "    fout = './../../data/era5/era5_monthly_2deg'+var+'.nc'\n",
    "    if os.path.exists(fout):\n",
    "        ds = xr.open_dataset('./../../data/era5/era5_monthly_2deg'+var+'.nc')\n",
    "        print(ds.time[-1].data,idvar,var,ds.time[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df82aa-7a30-42b4-8e30-b88f2d71ff03",
   "metadata": {},
   "source": [
    "- now take all the individual files & merge together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e12edc-8e30-4add-bf00-8215f4b034ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = []\n",
    "for idvar,dvar in enumerate(data_var):\n",
    "    var = dvar[27:-5]\n",
    "    file = './../../data/era5/era5_monthly_2deg'+var+'.nc'\n",
    "    if os.path.exists(fout):\n",
    "        ds = xr.open_dataset(file)\n",
    "        if idvar==0:\n",
    "            ds_all = ds\n",
    "        else:\n",
    "            for var in ds:\n",
    "                ds_all[var]=ds[var]\n",
    "        \n",
    "ds_all.to_netcdf('./../../data/era5/era5_monthly_2deg.nc')\n",
    "ds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ds size in GB {:0.2f}\\n'.format(ds_all.nbytes / 1e9))\n",
    "#ds_all.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for var in ds_all:\n",
    "    ds_all[var][-1,:,:].plot()\n",
    "    plt.show()\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for var in ds_all:\n",
    "    ds_all[var].mean({'latitude','longitude'}).plot()\n",
    "    plt.show()\n",
    "    input()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all.air_temperature_at_2_metres.mean({'latitude','longitude'}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b779824-4a16-4137-a78f-c2bf9af06c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
