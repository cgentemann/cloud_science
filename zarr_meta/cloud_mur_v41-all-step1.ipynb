{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fda77a-aa1c-4334-9e33-fedef0d4d3dc",
   "metadata": {},
   "source": [
    "# Creating consolidated metadata file for MUR SST\n",
    "\n",
    "# step 1: create list of files\n",
    "\n",
    "Step 2 is [here](https://github.com/cgentemann/cloud_science/blob/master/zarr_meta/cloud_mur_v41-all-step2.ipynb)\n",
    "\n",
    "NASA JPL PODAAC has put the entire [MUR SST](https://podaac.jpl.nasa.gov/dataset/MUR-JPL-L4-GLOB-v4.1) dataset on AWS cloud as individual netCDF files, **but all ~7000 of them are netCDF files.**\\ Accessing one file works well, but accessing multiple files is **very slow** because the metadata for each file has to be queried. Here, we create **fast access** by consolidating the metadata and accessing the entire dataset rapidly via zarr. More background on this project:\n",
    "[medium article](https://medium.com/pangeo/fake-it-until-you-make-it-reading-goes-netcdf4-data-on-aws-s3-as-zarr-for-rapid-data-access-61e33f8fe685) and in this [repo](https://github.com/lsterzinger/fsspec-reference-maker-tutorial). We need help developing documentation and more test datasets. If you want to help, we are working in the [Pangeo Gitter](https://gitter.im/pangeo-data/cloud-performant-netcdf4).\n",
    "\n",
    "\n",
    "To run this code:\n",
    "- you need to set your AWS credentials up using `aws configure --profile esip-qhub`\n",
    "- you need to set up your `.netrc` file in your home directory with your earthdata login info\n",
    "\n",
    "\n",
    "Authors:\n",
    "- [Chelle Gentemann](https://github.com/cgentemann)\n",
    "- [Rich Signell](https://github.com/rsignell-usgs)\n",
    "- [Lucas Steringzer](https://github.com/lsterzinger/)\n",
    "- [Martin Durant](https://github.com/martindurant)\n",
    "\n",
    "Credit:\n",
    "- Funding: Interagency Implementation and Advanced Concepts Team [IMPACT](https://earthdata.nasa.gov/esds/impact) for the Earth Science Data Systems (ESDS) program\n",
    "- AWS Public Dataset [Program](https://registry.opendata.aws/mur/)\n",
    "- [QuanSight](https://www.quansight.com/) for creating Qhub, [ESIP Labs ](https://www.esipfed.org/lab) for deploying it, and [AWS Sustainablity](https://aws.amazon.com/government-education/sustainability-research-credits/) for funding it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import requests\n",
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from json import dumps\n",
    "from io import StringIO\n",
    "from os.path import dirname, join\n",
    "import netrc\n",
    "import dask.bag as db\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import fsspec\n",
    "import ujson   # fast json\n",
    "from fsspec_reference_maker.hdf import SingleHdf5ToZarr \n",
    "from fsspec_reference_maker.combine import MultiZarrToZarr\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec_reference_maker\n",
    "fsspec_reference_maker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb21a74-8ca4-42ed-8f0e-97c881f3f4a5",
   "metadata": {},
   "source": [
    "- output file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ae004-5f33-41c6-98d5-461a8b1e74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = 's3://esip-qhub/nasa/mur/jsons_all/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-philip",
   "metadata": {},
   "source": [
    "## set up earthdata login credentials\n",
    "- code for setting up earthdata_login from [here](https://github.com/podaac/tutorials/blob/master/notebooks/cloudwebinar/cloud_direct_access_s3.py)\n",
    "- for the earthdata login to work you need to create a .netrc file on your home directory\n",
    "- .netrc file contains:\\\n",
    "machine urs.earthdata.nasa.gov\\\n",
    "login 'earthdata username'\\\n",
    "password 'password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Setting up earthdata login credentials \n",
    "# this code is from https://github.com/podaac/tutorials/blob/master/notebooks/cloudwebinar/cloud_direct_access_s3.py\n",
    "def setup_earthdata_login_auth(endpoint):\n",
    "    \"\"\"\n",
    "    Set up the request library so that it authenticates against the given Earthdata Login\n",
    "    endpoint and is able to track cookies between requests.  This looks in the .netrc file \n",
    "    first and if no credentials are found, it prompts for them.\n",
    "    Valid endpoints:\n",
    "        urs.earthdata.nasa.gov - Earthdata Login production\n",
    "    \"\"\"\n",
    "    try:\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        # FileNotFound = There's no .netrc file\n",
    "        # TypeError = The endpoint isn't in the netrc file, causing the above to try unpacking None\n",
    "        print(\"There's no .netrc file or the The endpoint isn't in the netrc file\")\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "###############################################################################\n",
    "edl=\"urs.earthdata.nasa.gov\"\n",
    "setup_earthdata_login_auth(edl)\n",
    "\n",
    "def begin_s3_direct_access():\n",
    "    url=\"https://archive.podaac.earthdata.nasa.gov/s3credentials\"\n",
    "    response = requests.get(url).json()\n",
    "    return s3fs.S3FileSystem(key=response['accessKeyId'],\n",
    "                             secret=response['secretAccessKey'],\n",
    "                             token=response['sessionToken'],\n",
    "                             client_kwargs={'region_name':'us-west-2'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eea16b-d7fc-4076-adef-9e286cc788b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import ebdpy as ebd\n",
    "\n",
    "ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "profile = 'esip-qhub'\n",
    "region = 'us-west-2'\n",
    "endpoint = f's3.{region}.amazonaws.com'\n",
    "ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "worker_max = 30\n",
    "client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "                                      region=region, use_existing_cluster=True,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                      environment='pangeo', worker_profile='Medium Worker', \n",
    "                                      propagate_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-secondary",
   "metadata": {},
   "source": [
    "## Create a list of all MUR files that are on the PODAAC Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fs = begin_s3_direct_access()\n",
    "flist = []\n",
    "for lyr in range(2002,2023):\n",
    "    for imon in range(1,13):\n",
    "        fstr = str(lyr)+str(imon).zfill(2)+'*.nc'\n",
    "        files = fs.glob(join(\"podaac-ops-cumulus-protected/\", \"MUR-JPL-L4-GLOB-v4.1\", fstr))\n",
    "        for file in files:\n",
    "            flist.append(file)\n",
    "print('total number of individual netcdf files:',len(flist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63db3-5d5b-4830-b079-3459baad06f4",
   "metadata": {},
   "source": [
    "- add s3 to filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "urls = [\"s3://\" + f for f in flist]\n",
    "\n",
    "so = dict(mode='rb', anon=True, default_fill_cache=False, default_cache_type='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa013c7-eaeb-429f-bd97-c56851cc0613",
   "metadata": {},
   "source": [
    "- link to filesystem - BE CAREFUL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b45f6-8969-42f5-be0b-1898457bac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs2 = fsspec.filesystem('s3', anon=False)  \n",
    "#If the directory exists, remove it (and all the files)\n",
    "try:\n",
    "    fs2.rm(json_dir, recursive=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(u):\n",
    "    with fs.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        p = u.split('/')\n",
    "        date = p[4][0:8] #p[3]\n",
    "        fname = p[4] #p[5]\n",
    "        outf = f'{json_dir}{date}.{fname}.json'\n",
    "        print(outf)\n",
    "        with fs2.open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf666e-2459-4a6a-af6d-509c387f3d6f",
   "metadata": {},
   "source": [
    "- Create all the individual files using dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = dask.compute(*[dask.delayed(gen_json)(u) for u in urls], retries=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b3a17-4865-4236-a78b-0328eafc9535",
   "metadata": {},
   "source": [
    "- Create list of all the individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-hotel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flist2 = fs2.ls(json_dir)\n",
    "furls = sorted(['s3://'+f for f in flist2])\n",
    "print(len(furls))\n",
    "furls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778c9ce-592a-4012-a1be-ed54b2c964a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
