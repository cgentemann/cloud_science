{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fda77a-aa1c-4334-9e33-fedef0d4d3dc",
   "metadata": {},
   "source": [
    "# Creating consolidated metadata file for MUR SST\n",
    "\n",
    "# step 1: create list of files\n",
    "\n",
    "Step 2 is [here](https://github.com/cgentemann/cloud_science/blob/master/zarr_meta/cloud_mur_v41-all-step2.ipynb)\n",
    "\n",
    "NASA JPL PODAAC has put the entire [MUR SST](https://podaac.jpl.nasa.gov/dataset/MUR-JPL-L4-GLOB-v4.1) dataset on AWS cloud as individual netCDF files, **but all ~7000 of them are netCDF files.**\\ Accessing one file works well, but accessing multiple files is **very slow** because the metadata for each file has to be queried. Here, we create **fast access** by consolidating the metadata and accessing the entire dataset rapidly via zarr. More background on this project:\n",
    "[medium article](https://medium.com/pangeo/fake-it-until-you-make-it-reading-goes-netcdf4-data-on-aws-s3-as-zarr-for-rapid-data-access-61e33f8fe685) and in this [repo](https://github.com/lsterzinger/fsspec-reference-maker-tutorial). We need help developing documentation and more test datasets. If you want to help, we are working in the [Pangeo Gitter](https://gitter.im/pangeo-data/cloud-performant-netcdf4).\n",
    "\n",
    "\n",
    "To run this code:\n",
    "- you need to set your AWS credentials up using `aws configure --profile esip-qhub`\n",
    "- in a cell run `pip install earthdata` and then restart your kernel.\n",
    "\n",
    "\n",
    "Authors:\n",
    "- [Chelle Gentemann](https://github.com/cgentemann)\n",
    "- [Rich Signell](https://github.com/rsignell-usgs)\n",
    "- [Lucas Steringzer](https://github.com/lsterzinger/)\n",
    "- [Martin Durant](https://github.com/martindurant)\n",
    "\n",
    "Credit:\n",
    "- Funding: Interagency Implementation and Advanced Concepts Team [IMPACT](https://earthdata.nasa.gov/esds/impact) for the Earth Science Data Systems (ESDS) program\n",
    "- AWS Public Dataset [Program](https://registry.opendata.aws/mur/)\n",
    "- [QuanSight](https://www.quansight.com/) for creating Qhub, [ESIP Labs ](https://www.esipfed.org/lab) for deploying it, and [AWS Sustainablity](https://aws.amazon.com/government-education/sustainability-research-credits/) for funding it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import requests\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from json import dumps\n",
    "from io import StringIO\n",
    "from os.path import dirname, join\n",
    "import dask.bag as db\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import fsspec\n",
    "import ujson   # fast json\n",
    "#from fsspec_reference_maker.hdf import SingleHdf5ToZarr \n",
    "#from fsspec_reference_maker.combine import MultiZarrToZarr\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec_reference_maker\n",
    "fsspec_reference_maker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb21a74-8ca4-42ed-8f0e-97c881f3f4a5",
   "metadata": {},
   "source": [
    "- output file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ae004-5f33-41c6-98d5-461a8b1e74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = 's3://esip-qhub/nasa/mur/jsons_all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71984075-f8df-4f64-b94f-2e6192d82d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthdata import Auth #, DataColletions, DataGranules, Accessor\n",
    "auth = Auth().login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_s3_direct_access():\n",
    "    url=\"https://archive.podaac.earthdata.nasa.gov/s3credentials\"\n",
    "    response = requests.get(url).json()\n",
    "    return s3fs.S3FileSystem(key=response['accessKeyId'],\n",
    "                             secret=response['secretAccessKey'],\n",
    "                             token=response['sessionToken'],\n",
    "                             client_kwargs={'region_name':'us-west-2'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eea16b-d7fc-4076-adef-9e286cc788b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import ebdpy as ebd\n",
    "\n",
    "ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "profile = 'esip-qhub'\n",
    "region = 'us-west-2'\n",
    "endpoint = f's3.{region}.amazonaws.com'\n",
    "ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "worker_max = 30\n",
    "client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "                                      region=region, use_existing_cluster=True,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                      environment='pangeo', worker_profile='Medium Worker', \n",
    "                                      propagate_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-secondary",
   "metadata": {},
   "source": [
    "## Create a list of all MUR files that are on the PODAAC Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fs = begin_s3_direct_access()\n",
    "flist = []\n",
    "for lyr in range(2003,2004):\n",
    "    for imon in range(1,2):\n",
    "        fstr = str(lyr)+str(imon).zfill(2)+'*.nc'\n",
    "        files = fs.glob(join(\"podaac-ops-cumulus-protected/\", \"MUR-JPL-L4-GLOB-v4.1\", fstr))\n",
    "        for file in files:\n",
    "            flist.append(file)\n",
    "print('total number of individual netcdf files:',len(flist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63db3-5d5b-4830-b079-3459baad06f4",
   "metadata": {},
   "source": [
    "- add s3 to filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "urls = [\"s3://\" + f for f in flist]\n",
    "\n",
    "so = dict(mode='rb', anon=True, default_fill_cache=False, default_cache_type='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa013c7-eaeb-429f-bd97-c56851cc0613",
   "metadata": {},
   "source": [
    "- link to filesystem - BE CAREFUL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b45f6-8969-42f5-be0b-1898457bac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs2 = fsspec.filesystem('s3', anon=False)  \n",
    "#If the directory exists, remove it (and all the files)\n",
    "#try:\n",
    "#    fs2.rm(json_dir, recursive=True)\n",
    "#except:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1e0d4-c7e5-4fca-8351-33d7fd8c93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import Union, BinaryIO\n",
    "import logging\n",
    "import numpy as np\n",
    "import h5py\n",
    "import zarr\n",
    "from zarr.meta import encode_fill_value\n",
    "import numcodecs\n",
    "\n",
    "lggr = logging.getLogger('h5-to-zarr')\n",
    "_HIDDEN_ATTRS = {  # from h5netcdf.attrs\n",
    "    \"REFERENCE_LIST\",\n",
    "    \"CLASS\",\n",
    "    \"DIMENSION_LIST\",\n",
    "    \"NAME\",\n",
    "    \"_Netcdf4Dimid\",\n",
    "    \"_Netcdf4Coordinates\",\n",
    "    \"_nc3_strict\",\n",
    "    \"_NCProperties\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f601c-e84f-4314-8500-f4a453e72393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHdf5ToZarr:\n",
    "    \"\"\"Translate the content of one HDF5 file into Zarr metadata.\n",
    "\n",
    "    HDF5 groups become Zarr groups. HDF5 datasets become Zarr arrays. Zarr array\n",
    "    chunks remain in the HDF5 file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h5f : file-like\n",
    "        Input HDF5 file as a binary Python file-like object (duck-typed, adhering\n",
    "        to BinaryIO is optional)\n",
    "    url : string\n",
    "        URI of the HDF5 file.\n",
    "    spec : int\n",
    "        The version of output to produce (see README of this repo)\n",
    "    inline_threshold : int\n",
    "        Include chunks smaller than this value directly in the output. Zero or negative\n",
    "        to disable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h5f: BinaryIO, url: str,\n",
    "                 spec=1, inline_threshold=100):\n",
    "        # Open HDF5 file in read mode...\n",
    "        lggr.debug(f'HDF5 file: {h5f}')\n",
    "        self.input_file = h5f\n",
    "        self.spec = spec\n",
    "        self.inline = inline_threshold\n",
    "        self._h5f = h5py.File(h5f, mode='r')\n",
    "\n",
    "        self.store = {}\n",
    "        self._zroot = zarr.group(store=self.store, overwrite=True)\n",
    "\n",
    "        self._uri = url\n",
    "        lggr.debug(f'HDF5 file URI: {self._uri}')\n",
    "\n",
    "    def translate(self):\n",
    "        \"\"\"Translate content of one HDF5 file into Zarr storage format.\n",
    "\n",
    "        This method is the main entry point to execute the workflow, and\n",
    "        returns a \"reference\" structure to be used with zarr/kerchunk\n",
    "\n",
    "        No data is copied out of the HDF5 file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary containing reference structure. \n",
    "        \"\"\"\n",
    "        lggr.debug('Translation begins')\n",
    "        self._transfer_attrs(self._h5f, self._zroot)\n",
    "        self._h5f.visititems(self._translator)\n",
    "        if self.inline > 0:\n",
    "            self._do_inline(self.inline)\n",
    "        if self.spec < 1:\n",
    "            return self.store\n",
    "        else:\n",
    "            for k, v in self.store.copy().items():\n",
    "                if isinstance(v, list):\n",
    "                    self.store[k][0] = \"{{u}}\"\n",
    "                else:\n",
    "                    self.store[k] = v.decode()\n",
    "            return {\n",
    "                \"version\": 1,\n",
    "                \"templates\": {\n",
    "                    \"u\": self._uri\n",
    "                },\n",
    "                \"refs\": self.store\n",
    "            }\n",
    "\n",
    "    def _do_inline(self, threshold):\n",
    "        \"\"\"Replace short chunks with the value of that chunk\n",
    "\n",
    "        The chunk may need encoding with base64 if not ascii, so actual\n",
    "        length may be larger than threshold.\n",
    "        \"\"\"\n",
    "        for k, v in self.store.copy().items():\n",
    "            if isinstance(v, list) and v[2] < threshold:\n",
    "                self.input_file.seek(v[1])\n",
    "                data = self.input_file.read(v[2])\n",
    "                try:\n",
    "                    # easiest way to test if data is ascii\n",
    "                    data.decode('ascii')\n",
    "                except UnicodeDecodeError:\n",
    "                    data = b\"base64:\" + base64.b64encode(data)\n",
    "                self.store[k] = data\n",
    "\n",
    "    def _transfer_attrs(self, h5obj: Union[h5py.Dataset, h5py.Group],\n",
    "                        zobj: Union[zarr.Array, zarr.Group]):\n",
    "        \"\"\"Transfer attributes from an HDF5 object to its equivalent Zarr object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h5obj : h5py.Group or h5py.Dataset\n",
    "            An HDF5 group or dataset.\n",
    "        zobj : zarr.hierarchy.Group or zarr.core.Array\n",
    "            An equivalent Zarr group or array to the HDF5 group or dataset with\n",
    "            attributes.\n",
    "        \"\"\"\n",
    "        print(h5obj.name)\n",
    "        if h5obj.name=='/time':\n",
    "            for n, v in h5obj.attrs.items():\n",
    "                print('time:',n,v)\n",
    "\n",
    "        for n, v in h5obj.attrs.items():\n",
    "            if n in _HIDDEN_ATTRS:\n",
    "                continue\n",
    "\n",
    "            # Fix some attribute values to avoid JSON encoding exceptions...\n",
    "            if isinstance(v, bytes):\n",
    "                v = v.decode('utf-8')\n",
    "            elif isinstance(v, (np.ndarray, np.number)):\n",
    "                if v.dtype.kind == 'S':\n",
    "                    v = v.astype(str)\n",
    "                if n == '_FillValue':\n",
    "#                    print(n,v,v.dtype) #this just prints out fill values that already exist\n",
    "                    v = encode_fill_value(v, v.dtype)\n",
    "                elif v.size == 1:\n",
    "                    v = v.flatten()[0]\n",
    "                    if isinstance(v, (np.ndarray, np.number)):\n",
    "                        v = v.tolist()\n",
    "                else:\n",
    "                    v = v.tolist()\n",
    "            elif isinstance(v, h5py._hl.base.Empty):\n",
    "                v = \"\"\n",
    "            if v == 'DIMENSION_SCALE':\n",
    "                continue\n",
    "            try:\n",
    "                zobj.attrs[n] = v\n",
    "            except TypeError:\n",
    "                lggr.exception(\n",
    "                    f'Caught TypeError: {n}@{h5obj.name} = {v} ({type(v)})')\n",
    "\n",
    "    def _translator(self, name: str, h5obj: Union[h5py.Dataset, h5py.Group]):\n",
    "        \"\"\"Produce Zarr metadata for all groups and datasets in the HDF5 file.\n",
    "        \"\"\"\n",
    "        refs = {}\n",
    "        if isinstance(h5obj, h5py.Dataset):\n",
    "            lggr.debug(f'HDF5 dataset: {h5obj.name}')\n",
    "            if h5obj.id.get_create_plist().get_layout() == h5py.h5d.COMPACT:\n",
    "                RuntimeError(\n",
    "                    f'Compact HDF5 datasets not yet supported: <{h5obj.name} '\n",
    "                    f'{h5obj.shape} {h5obj.dtype} {h5obj.nbytes} bytes>')\n",
    "                return\n",
    "\n",
    "            #\n",
    "            # check for unsupported HDF encoding/filters\n",
    "            #\n",
    "            if h5obj.scaleoffset:\n",
    "                raise RuntimeError(\n",
    "                    f'{h5obj.name} uses HDF5 scaleoffset filter - not supported by reference-maker')\n",
    "            if h5obj.compression in ('szip', 'lzf'):\n",
    "                raise RuntimeError(\n",
    "                    f'{h5obj.name} uses szip or lzf compression - not supported by reference-maker')\n",
    "            if h5obj.compression == 'gzip':\n",
    "                compression = numcodecs.Zlib(level=h5obj.compression_opts)\n",
    "            else:\n",
    "                compression = None\n",
    "            \n",
    "            # Add filter for shuffle\n",
    "            filters = []\n",
    "            if h5obj.shuffle:\n",
    "                filters.append(numcodecs.Shuffle(elementsize=h5obj.dtype.itemsize))\n",
    "\n",
    "            # Get storage info of this HDF5 dataset...\n",
    "            cinfo = self._storage_info(h5obj)\n",
    "            if h5py.h5ds.is_scale(h5obj.id) and not cinfo:\n",
    "                return\n",
    "\n",
    "            # Create a Zarr array equivalent to this HDF5 dataset...\n",
    "            #print(h5obj.name,h5obj.fillvalue)\n",
    "            #print(h5obj.shape[0])\n",
    "            if h5obj.name=='/time':\n",
    "                print('time no fill_value')\n",
    "                za = self._zroot.create_dataset(h5obj.name, shape=h5obj.shape,\n",
    "                                                dtype=h5obj.dtype,\n",
    "                                                chunks=h5obj.chunks or False,\n",
    "                                                fill_value=0,\n",
    "                                                compression=compression,\n",
    "                                                filters=filters,\n",
    "                                                overwrite=True)\n",
    "            else:\n",
    "                za = self._zroot.create_dataset(h5obj.name, shape=h5obj.shape,\n",
    "                                                dtype=h5obj.dtype,\n",
    "                                                chunks=h5obj.chunks or False,\n",
    "                                                fill_value=h5obj.fillvalue,\n",
    "                                                compression=compression,\n",
    "                                                filters=filters,\n",
    "                                                overwrite=True)\n",
    "            lggr.debug(f'Created Zarr array: {za}')\n",
    "            self._transfer_attrs(h5obj, za)\n",
    "\n",
    "            adims = self._get_array_dims(h5obj)\n",
    "            za.attrs['_ARRAY_DIMENSIONS'] = adims\n",
    "            lggr.debug(f'_ARRAY_DIMENSIONS = {adims}')\n",
    "\n",
    "            # Store chunk location metadata...\n",
    "            if cinfo:\n",
    "                for k, v in cinfo.items():\n",
    "                    if h5obj.fletcher32:\n",
    "                        logging.info(\"Discarding fletcher32 checksum\")\n",
    "                        v['size'] -= 4\n",
    "                    self.store[za._chunk_key(k)] = [self._uri, v['offset'], v['size']]\n",
    "\n",
    "        elif isinstance(h5obj, h5py.Group):\n",
    "            lggr.debug(f'HDF5 group: {h5obj.name}')\n",
    "            zgrp = self._zroot.create_group(h5obj.name)\n",
    "            self._transfer_attrs(h5obj, zgrp)\n",
    "\n",
    "    def _get_array_dims(self, dset):\n",
    "        \"\"\"Get a list of dimension scale names attached to input HDF5 dataset.\n",
    "\n",
    "        This is required by the xarray package to work with Zarr arrays. Only\n",
    "        one dimension scale per dataset dimension is allowed. If dataset is\n",
    "        dimension scale, it will be considered as the dimension to itself.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dset : h5py.Dataset\n",
    "            HDF5 dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List with HDF5 path names of dimension scales attached to input\n",
    "            dataset.\n",
    "        \"\"\"\n",
    "        dims = list()\n",
    "        rank = len(dset.shape)\n",
    "        if rank:\n",
    "            for n in range(rank):\n",
    "                num_scales = len(dset.dims[n])\n",
    "                if num_scales == 1:\n",
    "                    dims.append(dset.dims[n][0].name[1:])\n",
    "                elif h5py.h5ds.is_scale(dset.id):\n",
    "                    dims.append(dset.name[1:])\n",
    "                elif num_scales > 1:\n",
    "                    raise RuntimeError(\n",
    "                        f'{dset.name}: {len(dset.dims[n])} '\n",
    "                        f'dimension scales attached to dimension #{n}')\n",
    "                elif num_scales == 0:\n",
    "                    # Some HDF5 files do not have dimension scales. \n",
    "                    # If this is the case, `num_scales` will be 0.\n",
    "                    # In this case, we mimic netCDF4 and assign phony dimension names.\n",
    "                    # See https://github.com/fsspec/kerchunk/issues/41\n",
    "\n",
    "                    dims.append(f\"phony_dim_{n}\")\n",
    "        return dims\n",
    "\n",
    "    def _storage_info(self, dset: h5py.Dataset) -> dict:\n",
    "        \"\"\"Get storage information of an HDF5 dataset in the HDF5 file.\n",
    "\n",
    "        Storage information consists of file offset and size (length) for every\n",
    "        chunk of the HDF5 dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dset : h5py.Dataset\n",
    "            HDF5 dataset for which to collect storage information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            HDF5 dataset storage information. Dict keys are chunk array offsets\n",
    "            as tuples. Dict values are pairs with chunk file offset and size\n",
    "            integers.\n",
    "        \"\"\"\n",
    "        # Empty (null) dataset...\n",
    "        if dset.shape is None:\n",
    "            return dict()\n",
    "\n",
    "        dsid = dset.id\n",
    "        if dset.chunks is None:\n",
    "            # Contiguous dataset...\n",
    "            if dsid.get_offset() is None:\n",
    "                # No data ever written...\n",
    "                return dict()\n",
    "            else:\n",
    "                key = (0,) * (len(dset.shape) or 1)\n",
    "                return {key: {'offset': dsid.get_offset(),\n",
    "                              'size': dsid.get_storage_size()}}\n",
    "        else:\n",
    "            # Chunked dataset...\n",
    "            num_chunks = dsid.get_num_chunks()\n",
    "            if num_chunks == 0:\n",
    "                # No data ever written...\n",
    "                return dict()\n",
    "\n",
    "            # Go over all the dataset chunks...\n",
    "            stinfo = dict()\n",
    "            chunk_size = dset.chunks\n",
    "            for index in range(num_chunks):\n",
    "                blob = dsid.get_chunk_info(index)\n",
    "                key = tuple(\n",
    "                    [a // b for a, b in zip(blob.chunk_offset, chunk_size)])\n",
    "                stinfo[key] = {'offset': blob.byte_offset, 'size': blob.size}\n",
    "            return stinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870054e-b279-4acc-b0bb-58ab270170be",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = urls[0]\n",
    "with fs.open(u, **so) as infile:\n",
    "    h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "    txt = ujson.dumps(h5chunks.translate()).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703b7eb-03b2-48b2-8e0d-1045762d4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36455a96-3784-4fd1-a99f-867545f6a538",
   "metadata": {},
   "source": [
    "#output with fillvalue=0\n",
    "\"time\\\\/.zattrs\":\"{\\\\n    \\\\\"_ARRAY_DIMENSIONS\\\\\": [\\\\n        \\\\\"time\\\\\"\\\\n    ],\\\\n    \\\\\"axis\\\\\": \\\\\"T\\\\\",\\\\n    \\\\\"comment\\\\\": \\\\\"Nominal time of analyzed fields\\\\\",\\\\n    \\\\\"long_name\\\\\": \\\\\"reference time of sst field\\\\\",\\\\n    \\\\\"standard_name\\\\\": \\\\\"time\\\\\",\\\\n    \\\\\"units\\\\\": \\\\\"seconds since 1981-01-01 00:00:00 UTC\\\\\"\\\\n}\",\"time\\\\/0\":\"base64:EINhKQ==\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d600a-f558-4aeb-87a8-6c8df785b5fd",
   "metadata": {},
   "source": [
    "#output with fillvalue=h5obj.fillvalue\n",
    "time\\\\/.zarray\":\"{\\\\n    \\\\\"chunks\\\\\": [\\\\n        1\\\\n    ],\\\\n    \\\\\"compressor\\\\\": null,\\\\n    \\\\\"dtype\\\\\": \\\\\"<i4\\\\\",\\\\n    \\\\\"fill_value\\\\\": -2147483647,\\\\n    \\\\\"filters\\\\\": null,\\\\n    \\\\\"order\\\\\": \\\\\"C\\\\\",\\\\n    \\\\\"shape\\\\\": [\\\\n        1\\\\n    ],\\\\n    \\\\\"zarr_format\\\\\": 2\\\\n}\",\"time\\\\/.zattrs\":\"{\\\\n    \\\\\"_ARRAY_DIMENSIONS\\\\\": [\\\\n        \\\\\"time\\\\\"\\\\n    ],\\\\n    \\\\\"axis\\\\\": \\\\\"T\\\\\",\\\\n    \\\\\"comment\\\\\": \\\\\"Nominal time of analyzed fields\\\\\",\\\\n    \\\\\"long_name\\\\\": \\\\\"reference time of sst field\\\\\",\\\\n    \\\\\"standard_name\\\\\": \\\\\"time\\\\\",\\\\n    \\\\\"units\\\\\": \\\\\"seconds since 1981-01-01 00:00:00 UTC\\\\\"\\\\n}\",\"time\\\\/0\":\"base64:EINhKQ==\"}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7d092-ee76-4582-a4a4-5c0908460ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fc4c6-ea53-495a-bdc6-1dd4bbe0552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b2f1a-a32d-4744-baa0-ebafbd651103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from os.path import dirname, join\n",
    "fs = begin_s3_direct_access()\n",
    "files = fs.glob(\n",
    "    join(\"podaac-ops-cumulus-protected/\", \"MUR-JPL-L4-GLOB-v4.1\", \"2005*.nc\")\n",
    ")\n",
    "ds2 = xr.open_dataset(fs.open(files[0]),mask_and_scale=False,decode_cf=False)\n",
    "#ds2 = xr.open_mfdataset(paths=[fs.open(f) for f in files[0:2]],combine=\"by_coords\",mask_and_scale=False,decode_cf=False,chunks={\"time\": 1})\n",
    "ds2.close()\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c704a90-763d-42cd-9c7b-0d2efaa763f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist2 = fs2.ls(json_dir)\n",
    "furls = sorted(['s3://'+f for f in flist2])\n",
    "print(len(furls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcb078-e0e0-4a0f-bd11-cbdf8d436866",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist2 = fs2.ls(json_dir)\n",
    "furls = sorted(['s3://'+f for f in flist2])\n",
    "print(len(furls))\n",
    "furls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c8807-9377-4828-97fe-dbea19d1fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a472b-7eb8-419b-954e-840525f95fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = furls[0]\n",
    "so = dict(mode='rb', anon=True, default_fill_cache=False, default_cache_type='first')\n",
    "with fs2.open(u, **so) as infile:\n",
    "    infile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0773f-6b94-4506-8168-1dcf2e6764bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open_files(furls[0]) as ofs:\n",
    "    fo_list = [json.load(of) for of in ofs]\n",
    "fo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee7563-6c7b-4603-921e-c4753704cec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ed565-7de1-473d-927e-e7e87b429512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(u):\n",
    "    with fs.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        p = u.split('/')\n",
    "        date = p[4][0:8] #p[3]\n",
    "        fname = p[4] #p[5]\n",
    "        outf = f'{json_dir}{date}.{fname}.json'\n",
    "        print(outf)\n",
    "#        with fs2.open(outf, 'wb') as f:\n",
    "#            f.write(ujson.dumps(h5chunks.translate()).encode());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf666e-2459-4a6a-af6d-509c387f3d6f",
   "metadata": {},
   "source": [
    "- Create all the individual files using dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = dask.compute(*[dask.delayed(gen_json)(u) for u in urls[0:2]], retries=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b3a17-4865-4236-a78b-0328eafc9535",
   "metadata": {},
   "source": [
    "- Create list of all the individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73f46a-94c6-47d5-afa6-238b23081134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-hotel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flist2 = fs2.ls(json_dir)\n",
    "furls = sorted(['s3://'+f for f in flist2])\n",
    "print(len(furls))\n",
    "furls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778c9ce-592a-4012-a1be-ed54b2c964a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
